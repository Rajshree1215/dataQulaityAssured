# Data Quality Pipeline Configuration
# Optimized for large datasets (50M rows, 1000 columns)

# ============================================================================
# DATA SOURCE CONFIGURATION
# ============================================================================
file_path: data/raw/global_health_data.csv

# ============================================================================
# PROCESSING CONFIGURATION
# ============================================================================
# Chunk size for processing (rows per chunk)
# For 8 cores, 64GB RAM: Recommended 50,000 - 100,000 rows
# Adjust based on number of columns and available memory
chunk_size: 50000

# Enable parallel processing (uses multiprocessing)
parallel_processing: true

# Number of worker processes (null = auto-detect CPU cores - 1)
# For 8 cores, recommended: 6-7 workers
num_workers: 6

# ============================================================================
# VALIDATION CONFIGURATION
# ============================================================================
# Schema file location
schema_path: schemas/health_schema.json

# Validation result format: COMPLETE, SUMMARY, or BASIC
# COMPLETE: Full details (slower, larger output)
# SUMMARY: Aggregated statistics (recommended for large datasets)
# BASIC: Minimal information (fastest)
validation_result_format: SUMMARY

# Stop processing on first critical error
fail_on_error: false

# Skip validation for chunks (faster, profiling only)
skip_validation: false

# ============================================================================
# PROFILING CONFIGURATION
# ============================================================================
# Enable column profiling
enable_profiling: true

# Include percentile statistics (25th, 50th, 75th)
include_percentiles: true

# Include value counts for low-cardinality columns
include_value_counts: true

# Maximum unique values to include value counts for
max_unique_for_counts: 50

# ============================================================================
# GREAT EXPECTATIONS CONFIGURATION
# ============================================================================
# Generate DataDocs HTML reports
generate_datadocs: true

# Sample size for DataDocs (null = full dataset)
# For large datasets, use a sample to speed up DataDocs generation
datadocs_sample_size: 100000

# ============================================================================
# OUTPUT CONFIGURATION
# ============================================================================
# Output directories
output_dir: outputs

# Save individual chunk reports
save_chunk_reports: true

# Save overall summary report
save_overall_report: true

# Generate HTML summary report
generate_html_report: true

# ============================================================================
# PERFORMANCE OPTIMIZATION
# ============================================================================
# Use schema-optimized data types when reading CSV
use_schema_dtypes: true

# Low memory mode for pandas
low_memory: true

# Memory limit per worker (GB)
# Set to prevent OOM errors in parallel processing
memory_limit_per_worker_gb: 8

# ============================================================================
# LOGGING CONFIGURATION
# ============================================================================
# Logging level: DEBUG, INFO, WARNING, ERROR, CRITICAL
log_level: INFO

# Log to file
log_to_file: true

# Log file directory
log_dir: logs

# Log rotation (max size in MB)
log_max_size_mb: 100

# Number of backup log files
log_backup_count: 5

# ============================================================================
# ERROR HANDLING CONFIGURATION
# ============================================================================
# Continue processing after chunk errors
continue_on_chunk_error: true

# Save error details to separate files
save_error_details: true

# Maximum number of retries for failed chunks
max_retries: 2

# ============================================================================
# ADVANCED SETTINGS
# ============================================================================
# Estimate optimal chunk size based on available memory
auto_estimate_chunk_size: false

# Available memory for estimation (GB)
available_memory_gb: 64

# CSV parsing engine: 'c' (fast) or 'python' (slower but more robust)
csv_engine: c

# Date parsing
parse_dates: false

# Compression (if input file is compressed)
# Options: null, 'gzip', 'bz2', 'zip', 'xz'
compression: null

# ============================================================================
# MONITORING AND ALERTING
# ============================================================================
# Progress reporting interval (number of chunks)
progress_report_interval: 10

# Enable performance metrics
enable_performance_metrics: true

# Alert threshold for failed chunks (%)
alert_threshold_failed_chunks_pct: 10

# Alert threshold for failed expectations (%)
alert_threshold_failed_expectations_pct: 5